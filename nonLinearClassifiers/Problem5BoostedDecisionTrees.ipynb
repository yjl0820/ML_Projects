{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name of the classifiers: XGBoost \n",
    "\n",
    "Accoring to machinelearning mastery, XGBoost is an implementation of gradient boosted decision trees designed for speed and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_svmlight_file(\"a9a.txt\")\n",
    "X_test, y_test = load_svmlight_file(\"a9a.t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "    For each of learning algorithms, you will need to set various hyperparameters (e.g. For XGBoost: the\n",
    "tree method, max depth, number of weak classifiers, objective, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fit model on training data. Fit based on the default values of the hyperparameters for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default values of the hyperparameters:\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Default values of the hyperparameters:\\n\", model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Make predictions for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 84.83%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"\\nAccuracy: %.2f%%\" % (accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES: \n",
    "\n",
    "The list of hyperparameters and brief description of each hyperparameter you tuned in training, their default values, and the final hyperparameter settings you use to get the best result.\n",
    "    Parameters to be tuned for XGBoost:\n",
    "    1. n estimators : \n",
    "    2. max depth\n",
    "    3. lambda\n",
    "    4. learning rate\n",
    "    5. missing\n",
    "    6. objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 6. Tuning hyperparameters.\n",
    "Tune the hyperparameters by seting the list of values. This will allow different combination of parameters. Meaning that the more values, more time it will take to compute. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 200, 300]\n",
    "max_depth = [2, 3, 4, 5]\n",
    "reg_lambda = [0.0, 1.0]\n",
    "learning_rate = [0.05, 0.1, 0.2, 0.5]\n",
    "missing = [None, 0]\n",
    "objective = ('binary:logistic', 'binary:hinge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = []\n",
    "for n_estimate, depth, lam, rate, miss, obj in product(n_estimators, max_depth, reg_lambda, learning_rate, missing, objective):\n",
    "    hyperparameters.append(\n",
    "        [n_estimate, depth, lam, rate, miss, obj])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "\n",
    "Number of hyperparameters: We have about 384 parameters combination with the above tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in hyperparameters:\n",
    "    count+=1\n",
    "\n",
    "print (count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Time it took:\n",
    "It took me about 45 minutes to run 384 hyperparameters on mac os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "50\n",
      "100\n",
      "300\n",
      "350\n",
      "Best accuracy that we can get: \n",
      " nan\n",
      "\n",
      "The best model parameters: \n",
      " {'max_depth': 2, 'learning_rate': 0.5, 'missing': None, 'n_estimators': 200, 'reg_lambda': 1.0, 'objective': 'binary:logistic'}\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "count = 0\n",
    "long = [5,10,50,100,300,350]\n",
    "\n",
    "for parameter in hyperparameters: \n",
    "    parameters = {'n_estimators': parameter[0], 'max_depth': parameter[1], 'reg_lambda': parameter[2], 'learning_rate': parameter[3], 'missing': parameter[4], 'objective': parameter[5]}\n",
    "\n",
    "    model = XGBClassifier(n_estimators= parameter[0], max_depth= parameter[1], reg_lambda= parameter[2], learning_rate= parameter[3], missing= parameter[4], objective= parameter[5])\n",
    "                          \n",
    "\n",
    "    kfold = KFold()\n",
    "    cross_val_scores = cross_val_score(model, X_train, y_train, cv=kfold) #takes long\n",
    "    accuracy = cross_val_scores.mean() * 100\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "\n",
    "    if (best_accuracy < accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        best_model = parameters\n",
    "        \n",
    "    for j in long:\n",
    "        if count == j: \n",
    "             print(count)\n",
    "        \n",
    "        \n",
    "print(\"Best accuracy that we can get: \\n\", accuracy)\n",
    "print(\"\\nThe best model parameters: \\n\", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Analysis with new hyperparameter:\n",
    "\n",
    "After several trial, I've noticed that the learning rate should be in between 0.2 to 0.7. With learning rate 0.7 and other fixed values, it resulted in highest accuracy. Lower or higher values would lower the accuracy. Also accuracy reduced when 150 < n_estimators < 260 . Within the given interval the accuracy would fluctuate but would not decrease dramatically. I've noticed that values around 190 -200 and 240-250 result in higher accuracy. Also, deeper max_depth would significantly reduce the accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New values of the hyperparameters: \n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.7, max_delta_step=0, max_depth=2,\n",
      "              min_child_weight=1, missing=None, monotone_constraints='()',\n",
      "              n_estimators=249, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "##TRIAL\n",
    "new_model = XGBClassifier(n_estimators=249, max_depth=2, reg_lambda=1, learning_rate= 0.7, missing=None, objective='binary:logistic')\n",
    "\n",
    "new_model.fit(X_train, y_train)\n",
    "print(\"New values of the hyperparameters: \\n\", new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold()\n",
    "cross_val_scores = cross_val_score(new_model, X_train, y_train, cv=kfold)\n",
    "accuracy = cross_val_scores.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Model Stats\n",
      "\n",
      "Accuracy: \n",
      "  85.09260820638065\n",
      "Cross Validation Training Error Rate:\n",
      "  0.1490739179361935\n",
      "Test Error Rate: \n",
      " 0.15011362938394446\n"
     ]
    }
   ],
   "source": [
    "print (\"New Model Stats\")\n",
    "print(\"\\nAccuracy: \\n \", accuracy)\n",
    "\n",
    "print(\"Cross Validation Training Error Rate:\\n \",\n",
    "      1-cross_val_scores.mean())\n",
    "\n",
    "print(\"Test Error Rate: \\n\",\n",
    "      1-new_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy when hyperparameter (Default) :** 84.17%\n",
    "\n",
    "**Accuracy when hyperparameter (New) :** 85.09%\n",
    "\n",
    "| Hyperparameter | Default Value | New Value |\n",
    "| :- | -: | :-: |\n",
    " *n_estimators* | 200 | 249\n",
    " *max_depth* | 2 | 2\n",
    " *reg_lambda* | 1 | 1\n",
    " *learning_rate* | 0.5 | 0.7   \n",
    " *missing* | None | .None\n",
    " *objective* | binary:logistic | binary:logistic\n",
    "\n",
    "\n",
    "**Cross Validation Training Error Rate:** 0.1490739179361935\n",
    "\n",
    "**Test Error Rate:** 0.15011362938394446\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
